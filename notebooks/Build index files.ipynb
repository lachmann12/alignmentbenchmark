{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build alignment index files\n",
    "\n",
    "Here we build alignment index files for the aligners Salmon, kallisto, STAR, HISAT2, and BWA. In this configuration we process Ensemble genomes of human and mouse.\n",
    "\n",
    "When AWS is configured the constructed indeces are uploaded to a specified AWS bucket using AWS credentials.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "Building index files with BWA is very memory intensive. The compute instance might require large amounts of memory to perform the index construction. The maayanlab/alignmentbenchmark supports a script than can download precomputed index files at \"/alignment/scripts/load_index.sh\". \n",
    "\n",
    "- 50GB of memory\n",
    "- 200GB of free disk space\n",
    "- CPU with high thread count (16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /alignment\n",
    "\n",
    "# the thread count used by some of the alignment algorithms.\n",
    "# Not all aligners support multithreading when constructing index files\n",
    "num_threads=16\n",
    "\n",
    "mkdir -p reference\n",
    "mkdir -p index/salmon\n",
    "mkdir -p index/kallisto\n",
    "mkdir -p index/star/human_96\n",
    "mkdir -p index/star/mouse_96\n",
    "mkdir -p index/hisat2/human_96\n",
    "mkdir -p index/hisat2/mouse_96\n",
    "mkdir -p index/bwa/human_96\n",
    "mkdir -p index/bwa/mouse_96\n",
    "\n",
    "# set number of open files, needed for high thread count\n",
    "ulimit -n 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jupyter notebook supports saving generated index files into a cloud repository for later reuse. Downloading the index files is faster than rebuilding. For the successful upload of data to Amazon S3 a user account has to be configured and a bucket location specified. The Bucket name has to be unique in S3.\n",
    "\n",
    "Here the private AWS key and AWS id can be set. To do so replace the strings \"yourid4lKG0RY5v4tha+b7UjA4WO\" and \"yourkeyWOQvMIc71yE8kfswJ5WGs6KMmvrfkh\" and adjust the regoin is needed and the bucket. **When working with credentials special care is advised. When uploading code with credentials to public repositories the account can be compromised by malicious third parties.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p ~/.aws\n",
    "touch config credentials\n",
    "\n",
    "# never upload credentials to Github, these credentials will be abused instantaneously\n",
    "echo \"[default]\" > ~/.aws/credentials\n",
    "echo \"aws_access_key_id = yourid4lKG0RY5v4tha\" >> ~/.aws/credentials\n",
    "echo \"aws_secret_access_key = yourkeyWOQvMIc71yE8kfswJ5W\" >> ~/.aws/credentials\n",
    "\n",
    "echo \"[default]\" > ~/.aws/config\n",
    "echo \"region = us-east-1\" >> ~/.aws/config\n",
    "echo \"output = json\" >> ~/.aws/config\n",
    "\n",
    "# Bucket name has to be changed to existing bucket in AWS user space\n",
    "aws_bucket=\"alignmentworkbench\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will take a file and compress it before uploading it to a S3 bucket. For this to work the AWS credentials have to be set. The AWS credentials need S3 write access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveIndexS3(){\n",
    "    tar cf - ${1} | pigz > ${1}.tar.gz\n",
    "    aws s3 cp ${1}.tar.gz s3://${3}/${2}.tar.gz\n",
    "    rm ${1}.tar.gz\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script was written to build index files for GRCh38 and GRCm38 release 96 from Ensembl. This can be changed to other genomes and annotations. The downloads are triffered in parallel and the script will wait until all files are downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 3850\n",
      "[2] 3851\n",
      "    %%  TToottaall        %%  RReecceeiivveedd  %%  XXffeerrdd    AAvveerraaggee  SSppeeeedd      TTiimmee       T Tiimmee          TiTmiem e  C uCrurrernetn\n",
      "t \n",
      "                                                                D lDolaoda d  U pUlpolaoda d    T oTtoatla l    S pSepnetn t      L eLfetf t  S pSepeede\n",
      "d\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0     0   - -0: ----::---- :----: ----::---- :--:--:--- -   - - :0--:--     0[3] 3854\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0[4] 3856\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0[5] 3858\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0[6] 3860\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     03850\n",
      "100 28.6M  100 28.6M    0     0  6656k      0  0:00:04  0:00:04 --:--:-- 6656k\n",
      "100 42.4M  100 42.4M    0     0  7831k      0  0:00:05  0:00:05 --:--:-- 8740k\n",
      "100 49.4M  100 49.4M    0     0  5747k      0  0:00:08  0:00:08 --:--:-- 9710k\n",
      "100 65.6M  100 65.6M    0     0  2935k      0  0:00:22  0:00:22 --:--:-- 6001k\n",
      "[1]   Done                    curl ftp://ftp.ensembl.org/pub/release-96/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz -o reference/human_cdna_96.fa.gz\n",
      "[2]   Done                    curl ftp://ftp.ensembl.org/pub/release-96/fasta/mus_musculus/cdna/Mus_musculus.GRCm38.cdna.all.fa.gz -o reference/mouse_cdna_96.fa.gz\n",
      "[5]-  Done                    curl ftp://ftp.ensembl.org/pub/release-96/gtf/homo_sapiens/Homo_sapiens.GRCh38.96.gtf.gz -o reference/human_96.gtf.gz\n",
      "[6]+  Done                    curl ftp://ftp.ensembl.org/pub/release-96/gtf/mus_musculus/Mus_musculus.GRCm38.96.gtf.gz -o reference/mouse_96.gtf.gz\n",
      "3851\n",
      "3854\n",
      "100  840M  100  840M    0     0  2436k      0  0:05:53  0:05:53 --:--:-- 11.3M70k  0:06:52  0:05:15  0:01:37 1616k  775k   00::0035::0242  2 850956kk\n",
      "[3]-  Done                    curl ftp://ftp.ensembl.org/pub/release-96/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz -o reference/human_dna_96.fa.gz\n",
      "3856\n",
      "100  768M  100  768M    0     0  1462k      0  0:08:58  0:08:58 --:--:-- 7157k\n",
      "[4]+  Done                    curl ftp://ftp.ensembl.org/pub/release-96/fasta/mus_musculus/dna/Mus_musculus.GRCm38.dna.primary_assembly.fa.gz -o reference/mouse_dna_96.fa.gz\n",
      "3858\n",
      "3860\n"
     ]
    }
   ],
   "source": [
    "# fetch 96 release cDNA sequences.\n",
    "# This is the reference used by the transcript quantification methods\n",
    "# this will take some time depending on the network speed\n",
    "# the downloads are forked, sometimes the FTP server of ensembl is slow\n",
    "curl ftp://ftp.ensembl.org/pub/release-96/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz -o reference/human_cdna_96.fa.gz &\n",
    "curl ftp://ftp.ensembl.org/pub/release-96/fasta/mus_musculus/cdna/Mus_musculus.GRCm38.cdna.all.fa.gz -o reference/mouse_cdna_96.fa.gz &\n",
    "\n",
    "# fetch 96 whole genome sequences\n",
    "# This is the reference used by true aligners such as STAR and HISAT2\n",
    "curl ftp://ftp.ensembl.org/pub/release-96/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz -o reference/human_dna_96.fa.gz &\n",
    "curl ftp://ftp.ensembl.org/pub/release-96/fasta/mus_musculus/dna/Mus_musculus.GRCm38.dna.primary_assembly.fa.gz -o reference/mouse_dna_96.fa.gz &\n",
    "\n",
    "# fetch GTF files annotationg the raw DNA sequences\n",
    "# they are needed by the alignment algorithms STAR and HISAT2 to count transcript reads\n",
    "curl ftp://ftp.ensembl.org/pub/release-96/gtf/homo_sapiens/Homo_sapiens.GRCh38.96.gtf.gz -o reference/human_96.gtf.gz &\n",
    "curl ftp://ftp.ensembl.org/pub/release-96/gtf/mus_musculus/Mus_musculus.GRCm38.96.gtf.gz -o reference/mouse_96.gtf.gz &\n",
    "\n",
    "# the script is now waiting for the downloads to finish before continuing\n",
    "# the jobs -p command lists forked jobs (commands followed by &)\n",
    "for job in `jobs -p`\n",
    "do\n",
    "echo $job\n",
    "    wait $job || let \"FAIL+=1\"\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the files are downloaded the files need to be extracted for further processing. Some aligners can work with uncompressed reference genomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack some files that can't be used when zipped\n",
    "unpigz reference/human_cdna_96.fa.gz\n",
    "unpigz reference/human_dna_96.fa.gz\n",
    "unpigz reference/human_96.gtf.gz\n",
    "\n",
    "unpigz reference/mouse_cdna_96.fa.gz\n",
    "unpigz reference/mouse_dna_96.fa.gz\n",
    "unpigz reference/mouse_96.gtf.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index building process starts here, now that all the prerequisites are met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salmon run create index\n",
    "time salmon index -p $num_threads -t reference/human_cdna_96.fa -i index/salmon/salmon_human_96\n",
    "time salmon index -p $num_threads  -t reference/mouse_cdna_96.fa -i index/salmon/salmon_mouse_96\n",
    "\n",
    "\n",
    "# kallisto run create index\n",
    "time kallisto index -i index/kallisto/kallisto_human_96.idx reference/human_cdna_96.fa\n",
    "time kallisto index -i index/kallisto/kallisto_mouse_96.idx reference/mouse_cdna_96.fa\n",
    "\n",
    "\n",
    "# STAR run create index\n",
    "# we use the genomeSAsparseD flag, it will create a smaller index that will require less memory\n",
    "\n",
    "GENOME=\"reference/human_dna_96.fa\"\n",
    "GTF=\"reference/human_96.gtf\"\n",
    "OUTPUT=\"index/star/human_96\"\n",
    "STAR \\\n",
    "    --runMode genomeGenerate \\\n",
    "    --genomeDir $OUTPUT \\\n",
    "    --genomeFastaFiles $GENOME \\\n",
    "    --sjdbGTFfile $GTF \\\n",
    "    --runThreadN $num_threads \\\n",
    "    --genomeSAsparseD 2 \\\n",
    "    --genomeSAindexNbases 13\n",
    "\n",
    "GENOME=\"reference/mouse_dna_96.fa\"\n",
    "GTF=\"reference/mouse_96.gtf\"\n",
    "OUTPUT=\"index/star/mouse_96\"\n",
    "time STAR \\\n",
    "    --runMode genomeGenerate \\\n",
    "    --genomeDir $OUTPUT \\\n",
    "    --genomeFastaFiles $GENOME \\\n",
    "    --sjdbGTFfile $GTF \\\n",
    "    --runThreadN $num_threads \\\n",
    "    --genomeSAsparseD 2 \\\n",
    "    --genomeSAindexNbases 13\n",
    "\n",
    "\n",
    "# HISAT2 run create index\n",
    "time hisat2-build -p $num_threads reference/human_dna_96.fa index/hisat2/human_96/human\n",
    "time hisat2-build -p $num_threads reference/mouse_dna_96.fa index/hisat2/mouse_96/mouse\n",
    "\n",
    "\n",
    "# BWA run create index\n",
    "time bwa index -p index/bwa/human_96/human_96 reference/human_dna_96.fa\n",
    "time bwa index -p index/bwa/mouse_96/mouse_96 reference/mouse_dna_96.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the alignment step some algorithms will calculate RNA-Seq at the transcript level that will be mapped to gene counts using an R script. For this a mapping table is required. This can be constructed for human and mouse by running the R script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rscript --vanilla scripts/buildMapping.r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When all the index files are built they can be uploaded to AWS. For this the AWS settings have to be configured above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveIndexS3 \"index/salmon/salmon_human_96\" \"salmon_human_96\" $aws_bucket\n",
    "saveIndexS3 \"index/salmon/salmon_mouse_96\" \"salmon_mouse_96\" $aws_bucket\n",
    "\n",
    "saveIndexS3 \"index/kallisto/kallisto_human_96.idx\" \"kallisto_human_96.idx\" $aws_bucket\n",
    "saveIndexS3 \"index/kallisto/kallisto_mouse_96.idx\" \"kallisto_mouse_96\" $aws_bucket\n",
    "\n",
    "saveIndexS3 \"index/star/human_96\" \"star_human_96\" $aws_bucket\n",
    "saveIndexS3 \"index/star/mouse_96\" \"star_mouse_96\" $aws_bucket\n",
    "\n",
    "saveIndexS3 \"index/hisat2/human_96\" \"hisat2_human_96\" $aws_bucket\n",
    "saveIndexS3 \"index/hisat2/mouse_96\" \"hisat2_mouse_96\" $aws_bucket\n",
    "\n",
    "saveIndexS3 \"index/bwa/human_96\" \"bwa_human_96\" $aws_bucket\n",
    "saveIndexS3 \"index/bwa/human_96\" \"bwa_human_96\" $aws_bucket\n",
    "\n",
    "aws s3 cp supportfiles/gene_mapping.rda s3://${aws_bucket}/gene_mapping.rda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
